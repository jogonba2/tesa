{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Meeting 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Experiments for reducing the scores of negative candidates.\n",
    "* To finetune BART-Large instead of BART-CNN (lead bias?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental details:\n",
    "\n",
    "* I consider the same ranking size than the discriminative model of the paper (24 candidates).\n",
    "\n",
    "\n",
    "* The negative symbolic information is computed as $S^- = \\bigcup\\limits_{u\\in T}\\{v \\ /\\ (u, v) \\in \\textrm{E}\\ \\wedge \\not\\exists u' \\in T \\ : p(u', v) \\}$\n",
    "\n",
    "\n",
    "* I only used the category graph in the experimentation (not enough time for experimenting with the infobox graph).\n",
    "\n",
    "<br><ins>**Experiment 1**</ins>: to use $S^-$ as negative candidates. To maintain the ranking size, negative candidates are replaced randomly by the elements $s\\in S^-$ (only if $s \\not\\in G$ (exact match)). I considered different amounts of replaced negative candidates. \n",
    "\n",
    "\n",
    "| Replacements (%) | MAP | R@10 | MRR | Different negatives w.r.t baseline (%) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Baseline | **89.80** | **99.28** | 95.69 | 0% |\n",
    "| 25% | 89.66 | 99.07 | 96.02 | 12.75% |\n",
    "| 50% | 88.34 | 99.08 | 94.67 | 24.95% |\n",
    "| 75% | 88.59 | 97.78 | **96.10** | 34.34% |\n",
    "| 100% | 88.08 | 98.88 | 94.28 | 42.85% |\n",
    "\n",
    "\n",
    "Interesting case (Abdul Rahman, Hamid Karzai): \n",
    "<br>**G** $\\rightarrow$ ['men', 'afghan politicians', 'afghans']\n",
    "<br>**$S^-$** $\\rightarrow$ ['afghan exiles', 'arabic masculine given names', 'people from kandahar province', 'turkish masculine given names', 'presidents of afghanistan', 'iranian masculine given names', 'honorary knights grand cross of the order of st michael and st george', 'afghan sunni muslims', 'mujahideen members of the soviet–afghan war', 'karzai family', 'pakistani masculine given names', '2000s in afghanistan', '2010s in afghanistan', 'afghan expatriates in pakistan']\n",
    "\n",
    "<ins>**Experiment 2**</ins>: to use the positive symbolic information (LCA) $S^+$ as negative candidates. To maintain the ranking size, negative candidates are replaced randomly by the elements $s\\in S^+$ (only if $s \\not\\in G$ (exact match)). All the elements in $S^+$ are considered as negative candidates.\n",
    "\n",
    "Also, as the experiment with \"positive\" data augmentation of the previous meeting was with the generative system, I repeated the experimentation with the discriminative.\n",
    "\n",
    "| Candidate type | Information | MAP | R@10 | MRR |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Positives | Lowest common ancestors (k=6) | 88.20 | 99.34 | 94.09 |\n",
    "| Negatives | Lowest common ancestors (k=6) | 88.34 | 98.88  | 95.30 |\n",
    "\n",
    "\n",
    "<ins>**Experiment 3**</ins>: to use $S^+$ (LCA) as \"soft-candidates\". To maintain the ranking size, negative candidates are replaced randomly by the elements $s\\in S^+$ (only if $s \\not\\in G$ (exact match)). To avoid the imbalance, a maximum of 12 positive and soft-positive candidates are allowed.\n",
    "\n",
    "|Information | Soft-label | MAP | R@10 | MRR |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Lowest common ancestors (k=6) | 0. | 88.20 | 99.34 | 94.09 |\n",
    "| Lowest common ancestors (k=6) | 0.2 | 87.45 | 98.00 | 95.66 |\n",
    "| Lowest common ancestors (k=6) | 0.4 | 88.49 | 98.71 | 95.17 |\n",
    "| Lowest common ancestors (k=6) | 0.6 | 88.89 | 98.71 | 95.31 |\n",
    "| Lowest common ancestors (k=6) | 0.8 | 88.81 | 98.99 | 95.50 |\n",
    "| Lowest common ancestors (k=6) | 1. | 88.34 | 98.88  | 95.30 |\n",
    "\n",
    "Question: In all the experiments where I have used information from the graph, the performance is very similar to the baseline w/o graph information, so, could this information already be contained in the model parameters or in the background/context of the input? [[Language Models as Knowledge Bases?]](https://arxiv.org/pdf/1909.01066.pdf). The information in the category and infobox graphs could appear in the Wikipedia articles and: 1) the pre-training data of BART contains Wikipedia, 2) the backgrounds are extracted from those articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of whether the information in the graph is useful or not, is it still interesting for us to integrate the negative candidates in the training of the generative model?\n",
    "\n",
    "Generative: $p(y_1^I | x_1^J) = \\prod_{i=1}^{I} p(y_i | y_1^{i-1}, x_1^J)$\n",
    "<br>Discriminative: $p(y | x_1^J) = f_{sm}(u_J^\\intercal W + b)_y$\n",
    "\n",
    "Idea: to integrate a discriminative loss along with the generative loss.\n",
    "\n",
    "$\\mathcal{L} = \\mathcal{L_D} + \\mathcal{L_G}$\n",
    "<br>$\\mathcal{L_G} = $\n",
    "<br>$\\mathcal{L_D} = $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lead bias checking\n",
    "\n",
    "I assume that the pre-trained BART w/o finetuning on CNN/DM (BART-Large) does not suffer from lead bias, so, I repeated the experimentation with the discriminative model (w/o graph information), but finetuning BART-Large instead of BART-CNN.\n",
    "\n",
    "\n",
    "\n",
    "| BART | MAP | R@10 | MRR |\n",
    "| --- | --- | --- | --- |\n",
    "| CNN | **89.80** | **99.28** | 95.69 |\n",
    "| Large | 89.61 | 99.06 | 95.74 |\n",
    "\n",
    "\n",
    "### Is BART-CNN biased to the lead sentences? \n",
    "\n",
    "I prepared some code for visualizing the encoder/decoder attentions and compute the accumulated probability for each sentence (**BARTBias** notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation details of the experimentation with negative candidates:\n",
    "\n",
    "**symbolic_algo**: (\"positivos\") lowest_common_ancestors_graph, value_intersection_infobox. (\"negativos\") negatives_from_neighborhood\n",
    "\n",
    "**symbolic_format**: si no se especifica, input (si se especifica \"input\", también). Si \"target\", la información simbólica se usa como positivos. Si \"negative_targets_X%\", la información simbólica se usa como negativos para reemplazar el X% de negativos.\n",
    "\n",
    "Por tanto, se pueden hacer cosas como, usar symbolic_algo \"positivos\" (e.g. lowest_common_ancestors_graph) como distractores con symbolic_format=negative_targets_100, usar symbolic_algo \"positivos\" como gold standards con symbolic_format=target y usar symbolic_algo \"negativos\" como distractores con symbolic_format=negative_targets_X%.\n",
    "\n",
    "\n",
    "**soft-labels** en create_modeling_task para distinguir las etiquetas de información del grafo con las del corpus (\"partial_aggregation\" -2 como valor clave-). En finetune.sh se puede especificar el peso que se le da a cada etiqueta: {\"not_aggregation\": 0, \"aggregation\": 1, \"partial_aggregation\": 0.6}, se calcula la binary crossentropy entre la p(1|x) generada por la red (posición 1 de la softmax) y la soft-label."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

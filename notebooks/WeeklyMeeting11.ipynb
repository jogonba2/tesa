{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Meeting 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conditioning on \"perfect information\".\n",
    "* Considering the negative candidates in the generative model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning on \"perfect information\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the experiments where I have used information from the graph, the performance is very similar to the baseline w/o graph information. I tried to think about potential problems:\n",
    "\n",
    "1) The information of the Wikipedia graphs is not useful for the task (in spite of the analysis we made in the Weekly-Meeting 5).\n",
    "\n",
    "2) The graph information is redundant in our experimental setup:\n",
    "  * Already contained in the BART parameters (the pre-training data contains Wikipedia).\n",
    "  * Already contained in the background/context of the aggregatable instances (extracted from Wikipedia)\n",
    "    \n",
    "3) The heuristics we used to extract the information of the graphs are not well suited for the task ($k$ lowest common ancestors and intersections at $k$-hop) or we aren't adding correctly the information to the model (as inputs with different formattings and as targets with different labels).\n",
    "\n",
    "\n",
    "What happens if we use \"perfect information\" (gold aggregations) in the input? (Confirm that if the model has perfect information in the input, then it should not make mistakes by only copying/focusing in that information):\n",
    "\n",
    "$BCE\\textrm{<sep>}G_1, G_2, ..., G_M\\textrm{<sep>} \\rightarrow G_i | BCE$\n",
    "\n",
    "| Set | MAP | R@10 | MRR |\n",
    "| --- | --- | --- | --- |\n",
    "| Train+Test | 99.83 | 100.00 | 100.00 |\n",
    "| Train | 64.97 | 84.88 | 82.51 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative finetuning of a pretrained discriminative model\n",
    "\n",
    "\n",
    "**Experiment 1**: First finetune BART in the discriminative setup and then finetune again in the generative setup. I think that, without restrictions on the parameters, the second finetuning could move far away the parameters learned in the first finetuning, forgetting the discriminative information. **Question**: is $p(c \\in \\{0, 1\\}|x_1^T;y_1^T)$ useful for $p(y_1^T | x_1^T)$?\n",
    "\n",
    "**Experiment 2**: First finetune BART in the discriminative setup, initialize the weights for the generative setup and freeze the encoder weights (the decoder weights are freely trainable).\n",
    "\n",
    "**Experiment 3**: First finetune BART in the discriminative setup, initialize the weights for the generative setup and make gradual unfreezing on encoder and decoder layers simultaneously (2 layers are unfreezed at each epoch (12 layers and max-epoch=6).\n",
    "\n",
    "**Experiment 4**: in the previous experiment, the results are very low until the last epoch (all the layers are unfreezed) (29.27 MAP in the 5th epoch and 75.12 MAP in the last epoch). So, I increased the number of epochs (3 epochs more) to see how much the results are increased.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "| Experiment| MAP | R@10 | MRR |\n",
    "| --- | --- | --- | --- |\n",
    "| Baseline |\t83.07 | 93.02 | 93.90 |\n",
    "| Exp-1 | 83.70 | 92.76 | 94.19 |\n",
    "| Exp-2 | 22.75 | 49.60  | 26.06 |\n",
    "| Exp-3 | 75.12 | 89.83 | 89.44 |\n",
    "| Exp-4 | 82.61 | 92.57 | 94.34 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without a pretrained discriminative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three ways of incorporating negative candidates in the generative finetuning:\n",
    "\n",
    "* **Experiment 1**: Use negation to optimize the same generative objective than in all the previous experiments ($p_\\theta(y_1^T | x_1^T)$) for positive and negative aggregations.<br><br>\n",
    "\n",
    "     * *Train*: $x_1^T \\rightarrow \\left\\{\\begin{matrix}\n",
    "\\textrm{are}\\ C_i\\ \\textrm{<sep>}\\ x_1^T & \\textrm{G}(C_i)=1 \\\\\n",
    "\\textrm{are not}\\ C_i\\ \\textrm{<sep>}\\ x_1^T & \\textrm{G}(C_i)=0 \n",
    "\\end{matrix}\\right.$\n",
    "     <br><br>\n",
    "     * *Rank*: ranking the candidates by $p_\\theta(are\\ C_i\\ \\textrm{<sep>}\\ x_1^T | x_1^T)$ (\"rank the candidates by their probability of being the correct aggregation\")\n",
    "     <br><br>\n",
    "     * *Generate*: positive aggregations $x_1^T \\rightarrow \\textrm{are} $ ~~not~~ $\\_\\_\\_ \\textrm{<sep>}$,     negative aggregations $x_1^T \\rightarrow \\textrm{are not}\\ \\_\\_\\_ \\textrm{<sep>}$\n",
    "\n",
    "<br><br>\n",
    "* **Experiment 2**: Jointly optimizing a discriminative and generative losses.\n",
    "<br><br>\n",
    "\n",
    "  <img src=\"summary_all_models.jpg\">\n",
    "  <br><br>\n",
    "  \n",
    "     * *Train*: $\\mathcal{L} = \\textrm{G}(C_i)\\mathcal{L_G}\\ +\\ \\mathcal{L_D}$\n",
    "     * *Rank*: same than the discriminative model.\n",
    "     * *Generate*: same than the generative model.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "| Experiment | MAP | R@10 | MRR |\n",
    "| --- | --- | --- | --- |\n",
    "| Baseline |\t83.07 | 93.02 | 93.90 |\n",
    "| Exp-1 | 67.80 | 83.45 | 85.52 |\n",
    "| Exp-2 | |   |  |\n",
    "\n",
    "\n",
    "I also tried, in the Experiment-1, to format the target in a different way: change \"are not\" by a \"ยง\" symbol and remove \"are\". The results improve (76.50 MAP vs 67.80 MAP) but it is also lower than the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some cases in the Experiment-1:\n",
    "\n",
    "(Lovie Smith, Tony Dungy)\n",
    "\n",
    "**Context**: Lovie Lee Smith is an American football coach. He is the head football coach at the University of Illinois. He was previously the head coach of the Chicago Bears of the National Football League from 2004 to 2012, and the NFL's Tampa Bay Buccaneers from 2014 to 2015. Smith has been to the Super Bowl twice, as the defensive coordinator for the St. Louis Rams and as the head coach for the Bears in 2006. Anthony Kevin Dungy is a former professional American football player and coach in the National Football League . Dungy was head coach of the Tampa Bay Buccaneers from 1996 to 2001, and head coach of the Indianapolis Colts from 2002 to 2008. Dungy and Smith Are Proving Nice Coaches Can Finish First: I ca n't believe I 'm treated like a 12-year-old when I 'm 31,'' he told New York magazine in December. Smith and Dungy have a style that seems to fit the year-round nature of their sport. Lovie Smith, Tony Dungy\n",
    "<br>**Target**: are football coaches  ---> PROB= 0.787\n",
    "\n",
    "**Context**: Lovie Lee Smith is an American football coach. He is the head football coach at the University of Illinois. He was previously the head coach of the Chicago Bears of the National Football League from 2004 to 2012, and the NFL's Tampa Bay Buccaneers from 2014 to 2015. Smith has been to the Super Bowl twice, as the defensive coordinator for the St. Louis Rams and as the head coach for the Bears in 2006. Anthony Kevin Dungy is a former professional American football player and coach in the National Football League . Dungy was head coach of the Tampa Bay Buccaneers from 1996 to 2001, and head coach of the Indianapolis Colts from 2002 to 2008. Dungy and Smith Are Proving Nice Coaches Can Finish First: I ca n't believe I 'm treated like a 12-year-old when I 'm 31,'' he told New York magazine in December. Smith and Dungy have a style that seems to fit the year-round nature of their sport. Lovie Smith, Tony Dungy\n",
    "<br>**Target**: are former major league baseball players  ---> PROB= 0.779\n",
    "\n",
    "**Context**: Lovie Lee Smith is an American football coach. He is the head football coach at the University of Illinois. He was previously the head coach of the Chicago Bears of the National Football League from 2004 to 2012, and the NFL's Tampa Bay Buccaneers from 2014 to 2015. Smith has been to the Super Bowl twice, as the defensive coordinator for the St. Louis Rams and as the head coach for the Bears in 2006. Anthony Kevin Dungy is a former professional American football player and coach in the National Football League . Dungy was head coach of the Tampa Bay Buccaneers from 1996 to 2001, and head coach of the Indianapolis Colts from 2002 to 2008. Dungy and Smith Are Proving Nice Coaches Can Finish First: I ca n't believe I 'm treated like a 12-year-old when I 'm 31,'' he told New York magazine in December. Smith and Dungy have a style that seems to fit the year-round nature of their sport. Lovie Smith, Tony Dungy\n",
    "<br>**Target**: are politicians  ---> PROB= 0.790 (highest one)\n",
    "\n",
    "**Context**: Lovie Lee Smith is an American football coach. He is the head football coach at the University of Illinois. He was previously the head coach of the Chicago Bears of the National Football League from 2004 to 2012, and the NFL's Tampa Bay Buccaneers from 2014 to 2015. Smith has been to the Super Bowl twice, as the defensive coordinator for the St. Louis Rams and as the head coach for the Bears in 2006. Anthony Kevin Dungy is a former professional American football player and coach in the National Football League . Dungy was head coach of the Tampa Bay Buccaneers from 1996 to 2001, and head coach of the Indianapolis Colts from 2002 to 2008. Dungy and Smith Are Proving Nice Coaches Can Finish First: I ca n't believe I 'm treated like a 12-year-old when I 'm 31,'' he told New York magazine in December. Smith and Dungy have a style that seems to fit the year-round nature of their sport. Lovie Smith, Tony Dungy\n",
    "<br>**Target**: are japanese politicians  ---> PROB= 0.782"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "I go back to the error analysis of the generative system and I identified one thing that could be improved.\n",
    "If you see the rankings, typically longer candidates have lower probability (because it is computed as the product of the probability of each token), so, there are candidates whose the length could affect. These are some of the 30 most confused aggregations:\n",
    "\n",
    "* interested in the washington redskins [22/1] (in the context: washington redskins, ...) (MAP=19.20)\n",
    "* participants in a criminal case [11/1]  (in the context: murdered, convicted, victims, killer, ...) (MAP=12.90)\n",
    "* those associated with the new york yankees [16/1] (in the context: new york yankees, ...) (MAP=72.20)\n",
    "* men with ties to american politics [16/1] (in the context: politician, senator, united states, ...) (MAP=72.20)\n",
    "* republican former political appointees\n",
    "* involved in a potentially criminal scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

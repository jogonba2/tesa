{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "from pyvis import network as net\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import functools\n",
    "import pwb\n",
    "import pywikibot\n",
    "import wptools\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Wikipedia information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two sources of semantic structured knowledge: infoboxes and categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infoboxes\n",
    "\n",
    "\"Infobox templates contain important facts and statistics of a type which are common to related articles\". There are pre-defined infobox templates for many \"categories\" https://en.wikipedia.org/wiki/Wikipedia:List_of_infoboxes:\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Template:Infobox_poker_player\n",
    "* https://en.wikipedia.org/wiki/Template:Infobox_writer\n",
    "* https://en.wikipedia.org/wiki/Template:Infobox_officeholder\n",
    "\n",
    "An infobox can be seen as a RDF graph of subject:predicate:object triples.\n",
    "\n",
    "The predicates are similar for similar pages e.g. politicians, writers, and they can be used to identify a \"general\" type of an entity.\n",
    "\n",
    "Example for two politicians (property \"Prime Minister\" shared suggests that both have worked as politicians):\n",
    "\n",
    "        Nicolas Sarkozy : office : Minister of the Interior\n",
    "        Nicolas Sarkozy : Prime Minister : \tDominique de Villepin\n",
    "\n",
    "        François Bayrou : office : Minister of Justice\n",
    "        François Bayrou : Prime Minister : \tÉdouard Philippe\n",
    "\n",
    "Example for two countries with shared \"capital\" predicate:\n",
    "\n",
    "        France : capital : Paris\n",
    "        Italy : capital : Rome\n",
    "\n",
    "\n",
    "The objects identify more specific features of the entities:\n",
    "\n",
    "Example for two politicians, the value specifies the political party and even the \"procedence\", useful for inferring e.g. French Politicians\n",
    "\n",
    "        François Bayrou : Political party : Union for French Democracy\n",
    "\n",
    "        Nicolas Sarkozy : Political party : The Republicans (France)\n",
    "\n",
    "Example for two countries, ideally, the euro could be useful for inferring e.g. european countries and the coordinates could be useful for detecting the cardinal points of the location.\n",
    "\n",
    "        Italy : Currency : Euro (€)b (EUR)\n",
    "\n",
    "        France : Currency : Euro (€)b (EUR)\n",
    "\n",
    "        Italy : Coordinates : 41°54′N 12°29′E\n",
    "\n",
    "        France : Coordinates : 48°51′N 2°21′E\n",
    "\n",
    "Example for two corporations (computer companies):\n",
    "\n",
    "        Google : Industry : Computer software ...\n",
    "\n",
    "        Microsoft : Industry : Computer software ...\n",
    "\n",
    "\n",
    "https://pypi.org/project/wptools/0.2.1/ as parser for extracting the infoboxes (there aren't parsers of infoboxes\n",
    "for MediaWiki):\n",
    "\n",
    "        'conventional_long_name': 'French Republic',\n",
    "        'common_name': 'France',\n",
    "        'native_name': '{{native name|fr|République française|nbsp|=|omit}}',\n",
    "        'national_anthem': \"[[La Marseillaise]]\",\n",
    "        'capital': '[[Paris]]',\n",
    "        'coordinates': '{{Coord|48|51|N|2|21|E|type:city}}',\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have written code to get a graph of Wikipedia categories (a script for ***pywikibot [Wikipedia foundation]***). It is a DFS with depth 6 from the categories in the Wikipedia page of each entity in TESA. The search finishes if the maximum depth is exceeded or roots are reached ('culture', 'economy', 'education', ..., https://en.wikipedia.org/wiki/Category:Main_topic_classifications) or an artificial category is reached (\"wikipedia\", \"hidden\", \"category\", ...). There is an edge between u and v if v is a sub-category of u in Wikipedia. It recovers 73021 nodes and 209490 edges (similar to https://wikiworkshop.org/2019/papers/Wiki_Workshop_2019_paper_9.pdf). \n",
    "\n",
    "But the structure of categories in Wikipedia is not a hierarchy, there are cycles generated by human-artifacts that connect sub-categories to categories (Figure 2). So, we can't do things like computing the lowest common ancestor among entities' categories. Also, a category can have multiple parents (Figure 1) and there are multiple roots.\n",
    "\n",
    "It's a graph of categories that relates similar categories but not exactly a hierarchy as a whole.\n",
    "\n",
    "Also, large walks in the graph could relate categories in a \"stranger\" way as shown in Figure 3 (limiting the length of the walks alleviate the problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of multiple parents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900px\"\n",
       "            height=\"500px\"\n",
       "            src=\"_.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f8786758610>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./knowledge_graphs/category_graph_depth-6_dag.pkl\", \"rb\") as fr:\n",
    "    dag = pkl.load(fr)\n",
    "    \n",
    "with open(\"./knowledge_graphs/category_graph_depth-6.pkl\", \"rb\") as fr:\n",
    "    graph = pkl.load(fr)\n",
    "\n",
    "#graph = graph.reverse()\n",
    "\n",
    "subgraph = graph.subgraph([\"English politicians\"] + list(graph.successors(\"English politicians\")))\n",
    "\n",
    "print(\"Example of multiple parents\")\n",
    "g = net.Network(notebook=True, height=\"500px\", width=\"900px\", directed=True)\n",
    "g.show_buttons(filter_=['physics'])\n",
    "g.from_nx(subgraph)\n",
    "g.show(\"_.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of cycle that starts and ends in Rivers of Kern County, California (last category: San Joaquin River)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900px\"\n",
       "            height=\"500px\"\n",
       "            src=\"_.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f8786763110>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = nx.simple_cycles(graph)\n",
    "for i in range(110):\n",
    "    cycle = next(gen)\n",
    "#cycle = next(gen)\n",
    "cycle_subgraph = graph.subgraph(cycle) #[\"Bleeding Kansas\", \"Kansas Territory\"]) #cycle)\n",
    "\n",
    "print(\"Example of cycle that starts and ends in %s (last category: %s)\" % (cycle[0], cycle[-1]))\n",
    "\n",
    "g = net.Network(notebook=True, height=\"500px\", width=\"900px\", directed=True)\n",
    "g.show_buttons(filter_=['physics'])\n",
    "g.from_nx(cycle_subgraph)\n",
    "g.show(\"_.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to relate Taiwan with XML:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900px\"\n",
       "            height=\"500px\"\n",
       "            src=\"_.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f87867631d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key1 = \"Taiwan\"\n",
    "key2 = \"XML\"\n",
    "\n",
    "print(\"How to relate %s with %s:\\n\" % (key1, key2))\n",
    "\n",
    "g = net.Network(notebook=True, height=\"500px\", width=\"900px\", directed=True)\n",
    "g.show_buttons(filter_=['physics'])\n",
    "g.from_nx(graph.subgraph(next(nx.all_shortest_paths(graph, key1, key2))))\n",
    "g.show(\"_.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking Cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few \"recent\" works in the literature tried to generate a DAG from noisy knowledge graphs with cycles:\n",
    "\n",
    "* Efficient Pruning of Large Knowledge Graphs (https://www.ijcai.org/Proceedings/2018/0564.pdf):\n",
    "Given a (directed) noisy knowledge graph NKG(V;E)(hereafter denoted G for brevity), a set P \\subset V of terminological nodes and crumbs to be preserved (referred to  as protected nodes),  and  a  root r \\in P,  CRUMBTRAIL prunes G to obtain an acyclic subgraph G_P that contains all nodes of P, as well as possibly other nodes to guarantee connectivity properties as explained below...\n",
    "\n",
    "\n",
    "\n",
    "* Breaking Cycles in Noisy Hierarchies (https://par.nsf.gov/servlets/purl/10028167): Remove cycles while preserving the logical structure (hierarchy) of a directed graph as much as possible. Consider a ranking function f that assigns a ranking score to each node in the graph. A higher ranking score implies that the corresponding node is higher up (e.g., more general) in the hierarchy. Given such a ranking, the edges which violate the hierarchy (i.e., edges from a higher/general group to a lower/specic group) are potential candidates for removal. Thus in our approach, there are two sub-tasks involved:\n",
    "\n",
    "    * Inferring graph hierarchy (finding a ranking function)\n",
    "    * Proposing strategies to select violation edges as candidates for removal\n",
    "    \n",
    "Inferring graph hierarchy: The best performing and simple system is based in TrueSkill. It is a Bayesian skill rating system designed to calculate the relative skill of players from the set of generated competitions in multi-player games - Xbox Live rankings - . They transform a directed graph G=(V,E) into a multiplayer tournament with |V| players and |E| competitions. For each edge (u,v) \\in E, we consider that u loses the competition between u and v. Based on the current estimated skill levels of two players (u and v) and the outcome of a new game between them (edge(u,v)), the TrueSkill model updates the skill level μ and σ intuitively based on whether the outcome of the new competition is expected or unexpected.\n",
    "\n",
    "Selecting violation edges: Greedy, remove the edges that violates the hierarchy the most.\n",
    "\n",
    "I adapted the code of Breaking Cycles in Noisy Hierarchies (https://github.com/zhenv5/breaking_cycles_in_noisy_hierarchies) for python 3 and networkx 2.4 for creating a DAG of the categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there cycles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(len(list(nx.simple_cycles(dag)))>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute things like lowest common ancestors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowest_common_ancestors(graph, nodes, top_k):\n",
    "    assert nx.is_directed_acyclic_graph(graph), \"Graph has to be acyclic and directed.\"\n",
    "\n",
    "    common_ancestors = list(set.intersection(*[nx.ancestors(graph, node) for node in nodes if node != \"Living people\" or node != \"People\"]))\n",
    "\n",
    "    if not common_ancestors:\n",
    "        return []\n",
    "    \n",
    "    sum_of_path_lengths = np.zeros((len(common_ancestors)))\n",
    "    for ii, c in enumerate(common_ancestors):\n",
    "        sum_of_path_lengths[ii] = functools.reduce(lambda x, y: x + y, \n",
    "                                                   [nx.shortest_path_length(graph, c, node) \n",
    "                                                         for node in nodes])\n",
    "\n",
    "    indices = np.argsort(sum_of_path_lengths)[:top_k]   \n",
    "    return [common_ancestors[ii] for ii in indices], sum_of_path_lengths[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_common_descendants(graph, nodes, top_k):\n",
    "    assert nx.is_directed_acyclic_graph(graph), \"Graph has to be acyclic and directed.\"\n",
    "\n",
    "    common_descendants = list(set.intersection(*[nx.descendants(graph, node) for node in nodes]))\n",
    "\n",
    "    if not common_descendants:\n",
    "        return []\n",
    "    \n",
    "    sum_of_path_lengths = np.zeros((len(common_descendants)))\n",
    "    for ii, c in enumerate(common_descendants):\n",
    "        sum_of_path_lengths[ii] = functools.reduce(lambda x, y: x + y, \n",
    "                                                   [nx.shortest_path_length(graph, node, c) \n",
    "                                                         for node in nodes])\n",
    "\n",
    "    indices = np.argsort(sum_of_path_lengths)[:top_k]   \n",
    "    return [common_descendants[ii] for ii in indices], sum_of_path_lengths[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower common ancestors:\n",
      "\n",
      "(Cities in North America, Capitals): (['Cities', 'Municipalities', 'City'], array([2., 4., 4.])) \n",
      "\n",
      "\n",
      "(Barack Obama, Donald Trump): (['Living people', 'Presidents of the United States', 'American male non-fiction writers'], array([2., 2., 2.])) \n",
      "\n",
      "\n",
      "(French politicians, American politicians): (['Politicians', 'Political parties', 'Causes of death'], array([2., 4., 4.])) \n",
      "\n",
      "\n",
      "(Boeing, Aerospace companies of Europe): (['Aerospace companies', 'Transport companies', 'Spaceflight'], array([3., 5., 5.])) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some entities are also categories: Barack Obama, Donald Trump and Boeing\n",
    "keys1 = [\"Cities in North America\", \"Barack Obama\", \"French politicians\", \"Boeing\"]\n",
    "keys2 = [\"Capitals\", \"Donald Trump\", \"American politicians\", \"Aerospace companies of Europe\"]\n",
    "top_k = 3\n",
    "\n",
    "print(\"Lower common ancestors:\")\n",
    "for k1, k2 in zip(keys1, keys2):\n",
    "    print(\"\\n(%s, %s):\" % (k1, k2), lowest_common_ancestors(dag, [k1, k2], top_k=top_k), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of previous strange relationships do not appear in the DAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Taiwan, XML)\n",
      "\n",
      "(RAW) There is path\n",
      "(DAG) There isn't path\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "(Stalinism, Socialism)\n",
      "\n",
      "(RAW) There is path\n",
      "(DAG) There isn't path\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "(Entertainers, 1940s in Japan)\n",
      "\n",
      "(RAW) There is path\n",
      "(DAG) There is path\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "(Politicians, French politicians)\n",
      "\n",
      "(RAW) There is path\n",
      "(DAG) There is path\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keys1 = [\"Taiwan\", \"Stalinism\", \"Entertainers\", \"Politicians\"]\n",
    "keys2 = [\"XML\", \"Socialism\", \"1940s in Japan\", \"French politicians\"]\n",
    "\n",
    "\n",
    "for k1, k2 in zip(keys1, keys2):\n",
    "    print(\"(%s, %s)\\n\" % (k1, k2))\n",
    "    try:\n",
    "        nx.shortest_path(graph, k1, k2)\n",
    "        print(\"(RAW) There is path\")\n",
    "    except:\n",
    "        print(\"(RAW) There isn't path\")\n",
    "\n",
    "    try:\n",
    "        nx.shortest_path(dag, k1, k2)\n",
    "        print(\"(DAG) There is path\")\n",
    "    except:\n",
    "        print(\"(DAG) There isn't path\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "What information extract from the infoboxes and categories for conditioning the generation of BART? \n",
    "\n",
    "How to represent the information? Graph structures? Raw text? \n",
    "\n",
    "How to combine the information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infoboxes\n",
    "\n",
    "\n",
    "Given:\n",
    "\n",
    "* A tuple of entities E = {e1, ..., e_|E|}\n",
    "\n",
    "* The predicate:object pairs from the infobox of each entity T = {t_1, ..., t_|T| : t_i = {(p_i1, o_i1), ...}\n",
    "\n",
    "A graph G = (V, E) where V are objects and entities, and E = {(u, p, v) | u is an entity connected to the object v by means of a predicate p}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(graph, entities):\n",
    "    \n",
    "    def preprocess_predicate(pred):\n",
    "        # Remove numbers for multiple predicates (office, office2, ...)\n",
    "        pred = \"\".join(filter(lambda c: not c.isdigit(), pred))\n",
    "        return pred\n",
    "    \n",
    "    def preprocess_object(obj):\n",
    "        objs = re.findall(\"(\\[\\[.*?\\]\\])\", obj)\n",
    "        objs = [obj.split(\"|\")[0].split(\"#\")[0] for obj in objs]\n",
    "        objs = [obj.replace(\"[\", \"\").replace(\"]\", \"\") for obj in objs]\n",
    "        return objs\n",
    "        \n",
    "    def parse_infobox(infobox):\n",
    "        \n",
    "\n",
    "        def parse_coordinates(obj):\n",
    "            \"\"\"\n",
    "            The parser fails with some coordinate formats, but, if clean\n",
    "            different information can be extracted:\n",
    "                · Coordinates\n",
    "                · Region (ES, FR, GB, ..)\n",
    "            \"\"\"\n",
    "            obj = obj.replace(\"}\", \"\").replace(\"{\", \"\").split(\"|\")[1:]\n",
    "            \n",
    "            # region Coordinates #\n",
    "            coordinates = None\n",
    "            coord_delim = [obj.index(k) for k in [\"E\", \"W\"] if k in obj]\n",
    "            coord_delim = coord_delim[0] if coord_delim else None\n",
    "            \n",
    "            if coord_delim is not None:\n",
    "                coordinates = \" \".join(obj[:coord_delim + 1]) # Specify degrees\n",
    "            \n",
    "            # endregion Coordinates #\n",
    "            \n",
    "            # region CoordinatesRegion #\n",
    "            coord_region = None\n",
    "            for k in obj:\n",
    "                if \"region\" in k:\n",
    "                    coord_region = k.split(\":\")[1]\n",
    "                    break\n",
    "            # endregion CoordinatesRegion #\n",
    "            \n",
    "            return ((\"coordinates\", coordinates),\n",
    "                    (\"coordinates_region\", coord_region))\n",
    "            \n",
    "        pred_obj_pairs = []\n",
    "        for pred, obj in list(infobox.items()):\n",
    "            \n",
    "            # Treat manually-determined predicates (coordinates, ISIN for org, ...)\n",
    "            if pred == \"coordinates\":\n",
    "                coordinates_info = parse_coordinates(obj)\n",
    "                for (pred, obj) in coordinates_info:\n",
    "                    if obj is not None:\n",
    "                        pred_obj_pairs.append((pred, obj))\n",
    "            \n",
    "            # Extract pairs where the object is a reference to a resource in Wikipedia\n",
    "            if obj is not None and \"[[\" in obj:\n",
    "                objs = preprocess_object(obj)\n",
    "                pred = preprocess_predicate(pred)\n",
    "                for obj in objs:\n",
    "                    pred_obj_pairs.append((pred, obj))\n",
    "                    \n",
    "        return pred_obj_pairs     \n",
    "            \n",
    "    \n",
    "    triples = []\n",
    "    for entity in entities:\n",
    "        infobox = wptools.page(entity).get_parse().data[\"infobox\"]\n",
    "        pred_obj_pairs = parse_infobox(infobox)\n",
    "        triples.extend([(entity, obj, {\"label\": pred}) for (pred, obj) in pred_obj_pairs])\n",
    "    \n",
    "    graph.add_edges_from(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "en.wikipedia.org (parse) David Stern\n",
      "en.wikipedia.org (imageinfo) File:David Stern.jpg\n",
      "David Stern (en) data\n",
      "{\n",
      "  image: <list(1)> {'kind': 'parse-image', 'file': 'File:David Ste...\n",
      "  infobox: <dict(19)> name, image, order, office, term_start, term...\n",
      "  iwlinks: <list(3)> https://commons.wikimedia.org/wiki/Category:D...\n",
      "  pageid: 287571\n",
      "  parsetree: <str(39822)> <root><template><title>short description...\n",
      "  requests: <list(2)> parse, imageinfo\n",
      "  title: David Stern\n",
      "  wikibase: Q347958\n",
      "  wikidata_url: https://www.wikidata.org/wiki/Q347958\n",
      "  wikitext: <str(30624)> {{short description|American businessman,...\n",
      "}\n",
      "en.wikipedia.org (parse) Paul Tagliabue\n",
      "en.wikipedia.org (imageinfo) File:Paul Tagliabue crop.jpg\n",
      "Paul Tagliabue (en) data\n",
      "{\n",
      "  image: <list(1)> {'kind': 'parse-image', 'file': 'File:Paul Tagl...\n",
      "  infobox: <dict(15)> name, image, caption, office, term_start, te...\n",
      "  pageid: 272734\n",
      "  parsetree: <str(22545)> <root><template><title>short description...\n",
      "  requests: <list(2)> parse, imageinfo\n",
      "  title: Paul Tagliabue\n",
      "  wikibase: Q935259\n",
      "  wikidata_url: https://www.wikidata.org/wiki/Q935259\n",
      "  wikitext: <str(17744)> {{short description|7th Commissioner of t...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "info_graph = nx.DiGraph()\n",
    "entities = [\"David Stern\", \"Paul Tagliabue\"]\n",
    "f(info_graph, entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000px\"\n",
       "            height=\"700px\"\n",
       "            src=\"_.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f57aed8d810>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = net.Network(notebook=True, height=\"700px\", width=\"1000px\", directed=True)\n",
    "g.show_buttons(filter_=['physics'])\n",
    "g.from_nx(info_graph)\n",
    "g.show(\"_.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories\n",
    "\n",
    "Given:\n",
    "\n",
    "* A tuple of entities E = {e1, ..., e_|E|}\n",
    "* The categories of the wikipedia page of each entity, C = {c_1, ..., c_|C| : c_i = {c_i1, ..., c_ij, ...}}\n",
    "* A category graph G = (V, E) where V are the categories and E = {(u, v) | u!=v and v is a subcategory of u}\n",
    "\n",
    "A new graph G' = (V', E') is created by adding E to V (V') and (c_ij, e_i) edges to E (E').\n",
    "\n",
    "\n",
    "<img src=\"./graphfig.jpg\" width=500 height=400>\n",
    "\n",
    "\n",
    "Generalization (bottom-up traversal):\n",
    "\n",
    "* Lowest common ancestors of E \\in G'.\n",
    "\n",
    "\n",
    "* Subgraph induced by the shortest paths between the ancestors and the entities. It's like a more general picture of the entities in G'.\n",
    "\n",
    "\n",
    "* ...\n",
    "\n",
    "Specialization (top-down traversal):\n",
    "\n",
    "* Highest common descendants\n",
    "\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(entities, site, dag):\n",
    "    \n",
    "    def clean_category_title(raw_title):\n",
    "        for k in [\":Category:\", \"]\", \"[\", \"en:\"]:\n",
    "            raw_title = raw_title.replace(k, \"\")\n",
    "        return raw_title.split(\"|\")[0]\n",
    "    \n",
    "    for entity in entities:\n",
    "        page = list(site.search(entity, total=1, where=\"title\"))\n",
    "        if page:\n",
    "            page = page[0]\n",
    "            categories = list(page.categories())\n",
    "            categories = [clean_category_title(cat.title(as_link=True,\n",
    "                                                         textlink=True,\n",
    "                                                         with_ns=False)) for cat in categories]\n",
    "            \n",
    "            # Add entity and edges (category, entity) if category exists in dag\n",
    "            if not dag.has_node(entity):\n",
    "                dag.add_node(entity)\n",
    "            \n",
    "                for category in categories:\n",
    "                    if dag.has_node(category) and category not in entities:\n",
    "                        dag.add_edge(category, entity)\n",
    "    \n",
    "    return dag\n",
    "\n",
    "entities = [\"Paris\", \"France\"]\n",
    "#entities = [\"State Department\", \"Supreme Court\"]\n",
    "#entities = [\"Ségolène Royal\", \"François Bayrou\", \"Nicolas Sarkozy\"]\n",
    "site = pywikibot.Site()\n",
    "dag = f(entities, site, dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowest_common_ancestors(graph, nodes, top_k):\n",
    "    #assert nx.is_directed_acyclic_graph(graph), \"Graph has to be acyclic and directed.\"\n",
    "\n",
    "    common_ancestors = list(set.intersection(*[nx.ancestors(graph, node) for node in nodes if node != \"Living people\" or node != \"People\"]))\n",
    "\n",
    "    if not common_ancestors:\n",
    "        return []\n",
    "    \n",
    "    sum_of_path_lengths = np.zeros((len(common_ancestors)))\n",
    "    for ii, c in enumerate(common_ancestors):\n",
    "        sum_of_path_lengths[ii] = functools.reduce(lambda x, y: x + y, \n",
    "                                                   [nx.shortest_path_length(graph, c, node) \n",
    "                                                         for node in nodes])\n",
    "\n",
    "    indices = np.argsort(sum_of_path_lengths)[:top_k]   \n",
    "    return [common_ancestors[ii] for ii in indices], sum_of_path_lengths[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Paris, France)\n",
      "\n",
      "Ancestor: G20 nations, length: 5\n",
      "\n",
      "Ancestor: Southwestern European countries, length: 5\n",
      "\n",
      "Ancestor: Member states of the Union for the Mediterranean, length: 5\n",
      "\n",
      "Ancestor: Administrative territorial entities, length: 5\n",
      "\n",
      "Ancestor: Member states of the Organisation internationale de la Francophonie, length: 5\n",
      "\n",
      "Ancestor: Group of Eight nations, length: 5\n",
      "\n",
      "Ancestor: Member states of the Council of Europe, length: 5\n",
      "\n",
      "Ancestor: Republics, length: 5\n",
      "\n",
      "Ancestor: Geography of Europe, length: 5\n",
      "\n",
      "Ancestor: Use British English from April 2015, length: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "\n",
    "lowest_ancestors = lowest_common_ancestors(dag, entities, top_k=top_k)\n",
    "filtered_lower_ancestors = []\n",
    "print(\"(%s, %s)\\n\" % (entities[0], entities[1]))\n",
    "for ancestor, length in zip(*lowest_ancestors):\n",
    "    print(\"Ancestor: %s, length: %d\\n\" % (ancestor, length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgraph induced by the shortest paths among ancestors and entities\n",
      "Virginia and Vermont are States of the United States but Vermont is a New England state and Virginia not\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900px\"\n",
       "            height=\"500px\"\n",
       "            src=\"_.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff2c1424190>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Subgraph induced by the shortest paths among ancestors and entities\")\n",
    "print(\"Virginia and Vermont are States of the United States but Vermont is a New England state and Virginia not\")\n",
    "induced_vertices = []\n",
    "for ancestor in lowest_ancestors[0]:\n",
    "    for entity in entities:\n",
    "        induced_vertices.extend(nx.shortest_path(dag, ancestor, entity))\n",
    "\n",
    "g = net.Network(notebook=True, height=\"500px\", width=\"900px\", directed=True)\n",
    "g.show_buttons(filter_=['physics'])\n",
    "g.from_nx(dag.subgraph(induced_vertices))\n",
    "g.show(\"_.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the information\n",
    "\n",
    "\n",
    "Combine both infobox and category in one graph, labeling the categories edges with a predicate \"category\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Guardar edgelist para lanzar Breaking Cycles in Noisy Hierarchies\n",
    "labels = {}\n",
    "c = 0\n",
    "graph = graph.reverse()\n",
    "for node in graph.nodes():\n",
    "    if node not in labels:\n",
    "        labels[node] = c\n",
    "        c += 1\n",
    "\n",
    "with open(\"relabeling_indices.pkl\", \"wb\") as fw:\n",
    "    pkl.dump( {v: k for k, v in labels.items()}, fw)\n",
    "int_graph = nx.relabel_nodes(graph, labels)\n",
    "nx.write_edgelist(int_graph, \"tesa.edges\",data = False)\n",
    "\n",
    "with open(\"relabeling_indices.pkl\", \"rb\") as fr:\n",
    "    inverse_labeling = pkl.load(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditioning the generation\n",
    "\n",
    "\n",
    "Use the common ancestors of E as hierarchical labeling: \n",
    "\n",
    "  * A Hierarchical Neural Attention-based Text Classifier: https://www.aclweb.org/anthology/D18-1094.pdf\n",
    "    \n",
    "  * Hierarchical Multi-Label Classification Networks: http://proceedings.mlr.press/v80/wehrmann18a/wehrmann18a.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

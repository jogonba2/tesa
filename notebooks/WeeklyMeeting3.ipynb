{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Meeting 3\n",
    "---\n",
    "\n",
    "1. I reproduced the experimentation both with the generative and extractive systems of the paper and I integrated my code for fast inference.\n",
    "<br>\n",
    "\n",
    "2. Number of seen and unseen entities:\n",
    "        {\"test\": {\"seen\": 302, \"unseen\": 128} (70%/30%),\n",
    "         \"valid\": {\"seen\": 305, \"unseen\": 125 (71%/29%)} \n",
    "<br>\n",
    "                                     \n",
    "## Error Analysis (Generative)\n",
    "    \n",
    "Same errors that I showed you in the previous meeting\n",
    "    \n",
    "\n",
    "    \n",
    "Worse with locations, especially with unseen locations (the discriminative is also worse with locations than with persons and organizations)\n",
    "\n",
    "\n",
    "\n",
    "General aggregations are quite confused: men, countries, states, nations, americans, writers ...\n",
    "\n",
    "\n",
    "    \n",
    "The discriminative model does not make that confusions with general aggregations:\n",
    "    \n",
    "                 Agg                  Gen                 Disc               NRanks\n",
    "\n",
    "                 men                  102                    1                   13\n",
    "           countries                  118                    1                   44\n",
    "             nations                  111                    6                   40\n",
    "              states                   60                    0                    7\n",
    "           americans                   61                    6                   23\n",
    "             writers                   52                    2                   12\n",
    "             authors                   44                    1                    5\n",
    "             leaders                   33                    0                    3\n",
    "             political powers          39                    3                   41\n",
    "             regions                   34                    1                    5\n",
    "             groups                    14                    0                    1\n",
    "             medical foundations       12                    0                    1\n",
    "             foundations               11                    0                    1\n",
    "    \n",
    "\n",
    "    \n",
    "But the discriminative model confuses other aggregations (I think more specific):\n",
    "    \n",
    "                 Agg                  Gen                 Disc               NRanks\n",
    "                 \n",
    "              people involved.t.c       0                    8                    5\n",
    "              creators                  2                   10                    2\n",
    "              american entertainers     1                    6                    3\n",
    "              musical entertainers      0                    4                    1\n",
    "              conflicting leaders       0                    3                    1\n",
    "              european locations        2                   17                    1\n",
    "              european places           3                   16                    1\n",
    "              asian places              8                   19                    1\n",
    "              areas involved i.t.d      4                   14                    1\n",
    "              eastern countries         3                   10                    1\n",
    "              news agencies             4                    7                    1\n",
    "              corporations              1                    3                    3\n",
    "              transportation companies  0                    1                    1\n",
    "\n",
    "---\n",
    "\n",
    "Why men, countries and nations? -> why are used these general aggregations? is it because it's difficult\n",
    "      to aggregate some examples and they are used as \"wildcard\"? -> It's a mix, in some cases, \"men\" is not used\n",
    "      as a wildcard because the annotators identified more specific aggregations:\n",
    "      \n",
    "\n",
    "    Entities (person): Nanni Moretti, Silvio Berlusconi\n",
    "    8: public figures [0.013/1]\n",
    "    13: men [0.009/1]\n",
    "    16: italian [0.005/1]\n",
    "    17: media participants [0.003/1]\n",
    "\n",
    "    Entities (person): Louis J. Eppolito, Thomas Galpine\n",
    "    1: luchese crime family participants [0.046/1]\n",
    "    2: men on the plaintiff's side [0.038/1]\n",
    "    6: men [0.021/1]\n",
    "\n",
    "    Entities (person): Albert Einstein, Wolfgang Amadeus Mozart\n",
    "    1: geniuses [0.054/1]\n",
    "    3: historical figures [0.023/1]\n",
    "    5: men [0.013/1]\n",
    "    9: famous people [0.010/1]\n",
    "\n",
    "    Entities (person): John D. Negroponte, Pat Roberts\n",
    "    1: politicians [0.047/1]\n",
    "    3: american public servants [0.042/1]\n",
    "    9: statesmen [0.014/1]\n",
    "    13: men [0.009/1]\n",
    "\n",
    "        \n",
    "But, in other cases, it's complex to identify more specific aggregations and it acts like a wildcard \n",
    "      to increase the number of gold aggregations:\n",
    "        \n",
    "        \n",
    "    Entities (person): Robert Caro, Robert Moses \n",
    "    (the name is the only plausible aggregation)\n",
    "    6: american men named robert [0.016/1]\n",
    "    8: men [0.011/1]\n",
    "\n",
    "    Entities (person): Billy Burt Hopper, Bobby Y. Emory \n",
    "    (Hopper is a sherif and Emory a criminal)\n",
    "    3: convicted criminals [0.023/1]\n",
    "    7: men [0.015/1]\n",
    "\n",
    "    Entities (person): Edward L. Glaeser, Jon Gertner \n",
    "    (Glaeser is an economy professor and Gertner an historian)\n",
    "    1: economists [0.051/1]\n",
    "    11: men [0.004/1]\n",
    "\n",
    "    Entities (person): John J. Doherty, Martin F. Horn, Michael R. Bloomberg \n",
    "    (it's quite difficult to distinguish an aggregation in the context. Also, only Bloomberg has background)\n",
    "    3: new york city mayoral employees [0.027/1]\n",
    "    11: men [0.007/1]\n",
    "\n",
    "    Entities (person): Charles M. Kavanagh, Daniel Donohue\n",
    "    (church member and a student abused by Kavanagh)\n",
    "    3: church members [0.016/1]\n",
    "    14: men [0.007/1]\n",
    "      \n",
    "In almost any ranking with \"men\" as gold aggregation, the word \"man\" does not appear in the aggregatable instance. So, the system should be able to infer it e.g. from male forms of nouns or pronouns\n",
    "\n",
    "\n",
    "The use of \"countries\" as wildcard is frequent. The aggregation \"countries\" is even more frequent than the \"men\" aggregation (44 of 155 location rankings vs 13 of 248 person rankings):\n",
    "\n",
    "\n",
    "\n",
    "    Entities (location): Iran, Syria\n",
    "    4: nations [0.030/1]\n",
    "    7: countries [0.023/1]\n",
    "    There are more specific aggregations for that example (possibly fast annotation)\n",
    "    (confused by very reasonable aggregations: 1: western asian regions [0.051/0] 2: western asian areas [0.044/0])\n",
    "\n",
    "    Entities (location): Kosovo, Serbia:\n",
    "    6: countries [0.022/1]\n",
    "    7: nations [0.019/1]\n",
    "    8: republics [0.017/1]\n",
    "    Also it seems a fast annotation.\n",
    "\n",
    "    Entities (location): Canada, Turkey\n",
    "    4: nations [0.052/1]\n",
    "    5: countries [0.049/1]\n",
    "    I think general aggregations are suited for this example.\n",
    "\n",
    "    Entities (location): Israel, Lebanon\n",
    "    2: middle eastern countries [0.063/1]\n",
    "    5: nations [0.033/1]\n",
    "    6: countries [0.031/1]\n",
    "    I think there are also more specific aggregations (e.g. western asia)\n",
    "    \n",
    "    \n",
    "The word \"country\" appears exactly, in almost all cases, in the wikipedia backgrounds:\n",
    "\n",
    "---\n",
    "\n",
    "Is the generative system failing on detecting exact matches or lexical patterns? I thought this is related to the self-attention layers of the decoder.\n",
    "\n",
    "Possibly the discriminative system is able to access better to exact words because the decoder also receives the text of the aggregatable instance instead of only the gold aggregation. I hypothesized that relating the gold aggregation with the aggregatable instance in all the self-attention layers of the decoder (in addition to the encoder-decoder attention with the contextualized encoder output) is benefitial to detect lexical patterns and exact matches (as the uncontextual embeddings are considered in the first layer of the decoder).  I made an experiment to test this with the generative model. Basically to train the generative system to generate \"gold aggregation | aggregatable_instance\" (v2 target_format,). It's basically use the same input/output than the discriminative approach (without negative candidates) to finetune BART, and in inference, compute the probability of the sequence gold aggregation | aggregatable_instance. Modified beam search to efficiently generate aggregations until the separator symbol | during generation:\n",
    "\n",
    "(other ways to do this: modify BART to use the Q, K, V at all levels instead of the last contextualized):\n",
    "\n",
    "\n",
    "> * TESA Paper (Generative)\n",
    "> * average_precision: 0.70564 (+/-0.25775)\n",
    "> * precision_at_10: 0.24209 (+/-0.08664)\n",
    "> * recall_at_10: 0.90671 (+/-0.18503)\n",
    "> * ndcg_at_10: 0.79057 (+/-0.22204)\n",
    "> * reciprocal_best_rank: 0.84009 (+/-0.28662)\n",
    "> * reciprocal_average_rank: 0.35655 (+/-0.22191)\n",
    "\n",
    "> * Generative TF_V2\n",
    "> * average_precision: 0.82907 (+/-0.21078)\n",
    "> * precision_at_10: 0.25023 (+/-0.08964)\n",
    "> * recall_at_10: 0.93074 (+/-0.16163)\n",
    "> * ndcg_at_10: 0.88178 (+/-0.17535)\n",
    "> * reciprocal_best_rank: 0.93693 (+/-0.19078)\n",
    "> * reciprocal_average_rank: 0.42594 (+/-0.22274\n",
    "\n",
    "\n",
    "It shows better behaviour (12+ average precision, 3+ recall and 9+ rbr). Also it's the best system detecting\n",
    "locations (very similar to the discriminative).\n",
    "\n",
    "\n",
    "The confusions with men, countries, nations, are drastically reduced:\n",
    "\n",
    "* \"men\" confused by other 2 choices (appears in 13 rankings)\n",
    "* \"countries\" confused by other 2 choices (appears in 44 rankings)\n",
    "* \"nations\" confused by other 2 choices (appears in 40 rankings)\n",
    "\n",
    "Now, the model confuses very specific aggregations. These aggregations appear in few rankings but they are very lower in the rankings:\n",
    "\n",
    "* \"masjid al-taqwa attendees\" confused by other 21 choices (appears in 1 rankings)\n",
    "* \"interested in the washington redskins\" confused by other 21 choices (appears in 1 rankings)\n",
    "* \"participants in a crimal case\" confused by other 21 choices (appears in 1 rankings)\n",
    "* \"men who's first name begins with the letter r\" confused by other 21 choices (appears in 1 rankings)\n",
    "* \"former mlb professional baseball pichers\" confused by other 21 choices (appears in 1 rankings)\n",
    "* \"involved in a potentially criminal scheme\" confused by other 20 choices (appears in 1 rankings)\n",
    "* \"american men named robert\" confused by other 19 choices (appears in 1 rankings)\n",
    "\n",
    "Same factual inconsistencies in generation than the generative system of the paper (american politicians, but lower ranked):\n",
    "\n",
    "\n",
    "> 0. former French politicians | [0.059]\n",
    "> 1. french politicians | [0.057]\n",
    "> 2. politicians | [0.048]\n",
    "> 3. american politicians | [0.044]\n",
    "> 4. former french politicians | [0.042]\n",
    "> 5. former French presidents | [0.039]\n",
    "> 6. entertainers | [0.031]\n",
    "> 7. politic figures | [0.030]\n",
    "> 8. republicans | [0.029]\n",
    "> 9. former French politician | [0.029]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with larger rankings (more distractors)\n",
    "\n",
    "I evaluated the systems with 2x and 3x more distractors in the ranking (without training again with that number of candidates to make a fair comparison between the generative system and the discriminative - the generative is only trained with gold aggregations -):\n",
    "\n",
    "      Average-Precision\n",
    "---\n",
    "      System                24 (1x)    48 (2x)              72 (3x)\n",
    "                \n",
    "      Generative           70.55     61.66 (Δ8.89)        57.78 (Δ12.77)  \n",
    "      Discriminative       89.80     87.52 (Δ2.28)        83.00 (Δ6.80)\n",
    "      Δ                    19.25     25.86                25.22\n",
    "\n",
    "\n",
    "      Recall @10\n",
    "---\n",
    "      System                24 (1x)    48 (2x)              72 (3x)\n",
    "                \n",
    "      Generative           90.67     82.35 (Δ8.32)       77.11 (Δ13.56)\n",
    "      Discriminative       99.28     97.69 (Δ1.59)       96.82 (Δ2.46)\n",
    "      Δ                    8.61      15.34               19.71\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hierarchy of concepts\n",
    "\n",
    "Question: evaluate the generalization of the generated aggregations or the ranked aggregations?\n",
    "\n",
    "DBPedia classes:\n",
    "http://mappings.dbpedia.org/server/ontology/classes/\n",
    "I think it is quite \"flatten\" person -> politician (leaf)\n",
    "                                place -> settlement -> city (leaf)\n",
    "                                organization -> company (leaf)\n",
    "                           \n",
    "                                \n",
    "Wordnet *:\n",
    "\n",
    "hypernyms and hyponyms, I think it's quite suitable for the task (visual inspection). Some specializations\n",
    "are not synsets, but it is quite lexical -> american politicians, renowned artists, convicted criminals, ... (Qualifier+Noun) (convicted criminals <- criminals <- person) and the noun are synsets.\n",
    "https://www.cs.princeton.edu/courses/archive/spring07/cos226/assignments/wordnet.html\n",
    "\n",
    "\n",
    "Wikipedia:\n",
    "\n",
    "Api: https://en.wikipedia.org/w/api.php?action=query&list=categorymembers&cmnamespace=14&cmlimit=max&cmtitle=Category:main%20topic%20classifications\n",
    "\n",
    "Some human artifacts (members, list, .. by location, .. by location and occupation, ...\n",
    "I'm preparing the data for generate the \"modified\" graph from wikipedia.\n",
    "\n",
    "\n",
    "\n",
    "Evaluate the generalization on the hierarchy:\n",
    "\n",
    "Traverse the graph to compute distances that measure the generalization given a generated aggregation and a gold\n",
    "standard. Several measures, mostly based in the shortest path and information content are implemented in WordNet -> http://www.nltk.org/howto/wordnet.html, but:\n",
    "\n",
    "* path_similarity(\"politician\", \"dog\") = 0.14285714285714285\n",
    "* path_similarity(\"politician\", \"president\") = 0.14285714285714285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
